{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MALE402 2025 ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED: Fill in your details below:\n",
    "\n",
    "#Student Number         : 21930156\n",
    "#Surname                : Ngubane\n",
    "#First Name(s)          : Thabiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do NOT change any of the contents in this cell. \n",
    "#REQUIRED: You will just need to run the cell when needed\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Do NOT change any of the contents in this cell. \n",
    "#REQUIRED: You will just need to run the cell when needed\n",
    "\n",
    "#Read in the dataset\n",
    "df = pd.read_csv('Laptops2.csv')\n",
    "\n",
    "#Split the dataframe into training features (X) and the target feature (y) \n",
    "X = df.drop(columns=['Rating'], axis=1)\n",
    "y = df['Rating'] \n",
    " \n",
    "#Use sklearn's train_test_split to split the dataset into x_train, x_test, y_train, y_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:  84.67432950191571 %\n"
     ]
    }
   ],
   "source": [
    "#Do NOT change any of the contents in this cell. \n",
    "#REQUIRED: You will just need to run the cell when needed\n",
    "\n",
    "#Fit the LogisticRegression model to the data\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Display the accuracy of LogisticRegression model in a user friendly manner\n",
    "print(\"LogisticRegression: \", model.score(x_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **** ASSIGNMENT 2 QUESTIONS START HERE ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: 85.44 %\n",
      "DecisionTreeClassifier: 84.29 %\n",
      "LogisticRegression: 84.67 %\n",
      "VotingClassifier (Hard Voting): 87.36 %\n"
     ]
    }
   ],
   "source": [
    "#REQUIRED: Implement a VOTING CLASSIFIER using 'hard' voting\n",
    "\n",
    "#You must use the following information to complete this section:\n",
    "\n",
    "#1. Use these estimators for the voting classifier :\n",
    "#   GaussianNB(Parameters: none)\n",
    "#   DecisionTreeClassifier (Parameter: random_state=5)   \n",
    "#   LogisticRegression (Parameters: solver=\"lbfgs\", random_state=5)\n",
    "\n",
    "#2. Fit each of the classifiers and print the accuracy of all 3 models \n",
    "#   in a user friendly manner (Eg GaussianNB: 86.34 %)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# Instantiate models\n",
    "model_nb = GaussianNB()\n",
    "model_dt = DecisionTreeClassifier(random_state=5)\n",
    "model_lr = LogisticRegression(solver=\"lbfgs\", random_state=5, max_iter=1000)\n",
    "\n",
    "# Fit individual models\n",
    "model_nb.fit(x_train, y_train)\n",
    "model_dt.fit(x_train, y_train)\n",
    "model_lr.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate individual models\n",
    "print(f\"GaussianNB: {model_nb.score(x_test, y_test) * 100:.2f} %\")\n",
    "print(f\"DecisionTreeClassifier: {model_dt.score(x_test, y_test) * 100:.2f} %\")\n",
    "print(f\"LogisticRegression: {model_lr.score(x_test, y_test) * 100:.2f} %\")\n",
    "\n",
    "# Create Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[('gnb', model_nb),('dt', model_dt),('lr', model_lr)],voting='hard')\n",
    "\n",
    "# Fit and evaluate Voting Classifier\n",
    "voting_clf.fit(x_train, y_train)\n",
    "voting_accuracy = accuracy_score(y_test, voting_clf.predict(x_test)) * 100\n",
    "print(f\"VotingClassifier (Hard Voting): {voting_accuracy:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier: 84.29 %\n"
     ]
    }
   ],
   "source": [
    "#REQUIRED: Implement a STACKING CLASSIFIER\n",
    "\n",
    "#You must use the following information to complete this section:\n",
    "\n",
    "# 1. Base Estimators: DecisionTreeClassifier (Parameter: random_state=5) and SVC (Parameter: random_state=5)\n",
    "#    Final Estimator: LogisticRegression(Parameters: None)\n",
    "\n",
    "# 2. Fit the classifier and print the accuracy of the model\n",
    "#    in a user friendly manner (Eg StackingClassifier 87.76 %)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "# Define base estimators\n",
    "base_estimators = [('dt', DecisionTreeClassifier(random_state=5)),('svc', SVC(random_state=5, probability=True))]\n",
    "\n",
    "# Define final estimator\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# Create StackingClassifier\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators,final_estimator=final_estimator)\n",
    "\n",
    "# Fit the model\n",
    "stacking_clf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "accuracy = accuracy_score(y_test, stacking_clf.predict(x_test)) * 100\n",
    "print(f\"StackingClassifier: {accuracy:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging: Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier: 90.04 %\n"
     ]
    }
   ],
   "source": [
    "#REQUIRED: Implement a BAGGING CLASSIFIER\n",
    "\n",
    "#You must use the following information to complete this section:\n",
    "# 1. Parameters: estimator is DecisionTreeClassifier (with class_weight = 'balanced', random_state=5), \n",
    "#               max samples = 1000\n",
    "# 2. Fit the classifier and print the accuracy of the model\n",
    "#    in a user friendly manner (Eg BaggingClassifier 90.54 %)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Define base estimator\n",
    "base_estimator = DecisionTreeClassifier(class_weight='balanced', random_state=5)\n",
    "\n",
    "# Create Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(estimator=base_estimator,n_estimators=10, max_samples=1000,random_state=5)\n",
    "\n",
    "# Fit the classifier\n",
    "bagging_clf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, bagging_clf.predict(x_test)) * 100\n",
    "print(f\"BaggingClassifier: {accuracy:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_samples: 707\n"
     ]
    }
   ],
   "source": [
    "#REQUIRED: Implement HYPERPARAMETER TUNING using Randomized Search on BaggingClassifier\n",
    "\n",
    "#You must use the following information to complete this section:\n",
    "\n",
    "# 1. Parameters: estimator is BaggingClassifier with n_estimators=100, param_distribution \n",
    "# consists of max_samples in the range 500 to 1300 (inclusive)\n",
    "\n",
    "# 2. Fit the classifier and print the best max_samples hyperparameter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Define base estimator\n",
    "base_estimator = DecisionTreeClassifier(class_weight='balanced', random_state=5)\n",
    "\n",
    "# Define BaggingClassifier\n",
    "bagging = BaggingClassifier(estimator=base_estimator,n_estimators=100,random_state=5)\n",
    "\n",
    "# Define parameter distribution for max_samples\n",
    "param_dist = {'max_samples': np.arange(500, 1301)}\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search = RandomizedSearchCV(estimator=bagging,param_distributions=param_dist, n_iter=10,cv=5,random_state=5,n_jobs=-1)\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Display the best max_samples value\n",
    "best_max_samples = random_search.best_params_['max_samples']\n",
    "print(f\"Best max_samples: {best_max_samples}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging: Hyperparameter Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier: 92.72 %\n"
     ]
    }
   ],
   "source": [
    "#REQUIRED. Implement a HYPERPARAMETER TUNED BAGGING CLASSIFIER \n",
    "\n",
    "#You must use the following information to complete this section:\n",
    "\n",
    "# 1. Parameters: Parameters: estimator is DecisionTreeClassifier (with class_weight = 'balanced', random_state=5),\n",
    "#                max_samples = The value of the best max_samples from the hyperparameter tuning\n",
    "#                NOTE: You must pass the max_samples value obtained from the best_params_bg \n",
    "#                dictionary and not hardcode the value\n",
    "# 2. Fit the classifier and print the accuracy of the model\n",
    "#    in a user friendly manner (Eg BaggingClassifier 94.63 %)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Create best_params_bg dictionary from the random_search results\n",
    "best_params_bg = random_search.best_params_\n",
    "\n",
    "# Extract the best max_samples from best_params_bg\n",
    "best_max_samples = best_params_bg['max_samples']\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(class_weight='balanced', random_state=5)\n",
    "\n",
    "# Create BaggingClassifier with tuned parameter\n",
    "bagging_tuned = BaggingClassifier(estimator=base_estimator,n_estimators=100,max_samples=best_max_samples,random_state=5)\n",
    "\n",
    "# Fit the model\n",
    "bagging_tuned.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = bagging_tuned.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "# Print accuracy in user-friendly format\n",
    "print(f\"BaggingClassifier: {accuracy:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier: 90.04 %\n"
     ]
    }
   ],
   "source": [
    "#REQUIRED: Implement a BOOSTING CLASSIFIER using AdaBoostClassifier\n",
    "\n",
    "#You must use the following information to complete this section:\n",
    "\n",
    "# 1. Parameters: base_estimator is DecisionTreeClassifier (with max_depth=4 and class_weight = 'balanced',random_state=5)\n",
    "#                n_estimators=400, learning_rate = 0.5\n",
    "\n",
    "# 2. Fit the classifier and print the accuracy of the model\n",
    "#    in a user friendly manner (Eg AdaBoostClassifier 90.03%)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=4,class_weight='balanced',random_state=5)\n",
    "\n",
    "# Define the AdaBoostClassifier with required parameters\n",
    "adaboost_clf = AdaBoostClassifier(estimator=base_estimator,n_estimators=400,learning_rate=0.5,random_state=5)\n",
    "\n",
    "# Fit the model\n",
    "adaboost_clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = adaboost_clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "# Print accuracy in user-friendly format\n",
    "print(f\"AdaBoostClassifier: {accuracy:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Write as comments below a comprehensive report on the process in solving this  \n",
    "# machine learning problem, especially in the context of the observed results.\n",
    "\n",
    "#NOTE: Marks will be allocated for YOUR observations that go beyond just stating the results but\n",
    "# rather putting the results in the context of ensemble learning and hyperparameter tuning.\n",
    "# The use of generative AI towards your solutions may be considered plagiarism and as such\n",
    "# the plagiarism policy may be implemented.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Comprehensive Report on Solving the Machine Learning Problem\n",
    "\n",
    "# This project focused on predicting laptop ratings using various machine learning models,\n",
    "# with an emphasis on ensemble learning techniques and hyperparameter tuning.\n",
    "\n",
    "# The initial baseline model, Logistic Regression, achieved moderate accuracy (~84.67%),\n",
    "# indicating that the problem has some complexity beyond what a single linear model can capture.\n",
    "\n",
    "# To improve performance, ensemble methods were applied, leveraging the strengths of multiple models:\n",
    "\n",
    "# 1. Voting Classifier (Hard Voting):\n",
    "#    Combined three diverse classifiers—Gaussian Naive Bayes, Decision Tree, and Logistic Regression.\n",
    "#    This method capitalizes on the principle that multiple models can correct each other’s mistakes,\n",
    "#    resulting in improved robustness and accuracy (~87.36%). This confirms the value of aggregating different perspectives.\n",
    "\n",
    "# 2. Stacking Classifier:\n",
    "#    Used Decision Tree and Support Vector Classifier as base learners with Logistic Regression as the meta-learner.\n",
    "#    Although stacking is a powerful technique that can model complex decision boundaries,\n",
    "#    the observed accuracy (~84.29%) was slightly lower than voting, possibly due to insufficient diversity\n",
    "#    among base learners or lack of fine-tuning for the meta-model.\n",
    "\n",
    "# 3. Bagging Classifier:\n",
    "#    This method involves training multiple Decision Trees on bootstrapped samples, reducing variance.\n",
    "#    The Bagging Classifier achieved a notable improvement (~90.04%), highlighting how ensemble averaging stabilizes predictions.\n",
    "\n",
    "# 4. Hyperparameter Tuning with Randomized Search:\n",
    "#    Focused on optimizing the max_samples parameter in the Bagging Classifier.\n",
    "#    The tuning process selected an optimal value of 707 samples per estimator, boosting accuracy further (~92.72%).\n",
    "#    This underscores the critical role hyperparameter tuning plays in extracting maximal model performance,\n",
    "#    balancing bias-variance trade-offs effectively.\n",
    "\n",
    "# 5. Boosting Classifier (AdaBoost):\n",
    "#    By sequentially focusing on misclassified instances, AdaBoost reduced both bias and variance.\n",
    "#    Achieving ~90.04% accuracy, it provided comparable performance to bagging but via a complementary approach.\n",
    "\n",
    "# Key Observations:\n",
    "# - Ensemble methods consistently outperformed single classifiers, demonstrating their ability to improve generalization.\n",
    "# - Bagging and Boosting address model variance and bias differently, both leading to strong results.\n",
    "# - Hyperparameter tuning was essential for enhancing bagging, emphasizing the need for systematic model optimization.\n",
    "# - Stacking’s lower performance suggests careful selection and tuning of base and meta learners is vital.\n",
    "# - The quality of base estimators and diversity among them strongly influences ensemble success.\n",
    "\n",
    "# Conclusion:\n",
    "# Applying ensemble learning combined with targeted hyperparameter tuning significantly improved the prediction accuracy,\n",
    "# with the tuned Bagging Classifier performing best. These results highlight that leveraging multiple models and\n",
    "# optimizing their parameters is a powerful strategy for complex classification problems like laptop rating prediction.\n",
    "\n",
    "# Recommendations:\n",
    "# - Future work should include expanded hyperparameter tuning across all models.\n",
    "# - Incorporate feature engineering and selection to improve input quality.\n",
    "# - Evaluate models with cross-validation for more reliable performance estimates.\n",
    "# - Explore other ensemble methods and meta-learning architectures to further enhance results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
